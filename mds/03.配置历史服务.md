# 3. 配置历史服务


之前在执行spark-submit提交任务的时候，任务执行之后，UI就关闭了，我们看不到监控页面。

但是我们想看一下之前执行的历史任务记录怎么办，我们可以做以下配置；


## 修改配置文件

```text
mv spark-defaults.conf.template spark-defaults.conf
```


## 配置日志存储路径

```shell script
spark.eventLog.enable=true
spark.eventLog.dir=hdfs://node1:8020/directory
```

> 以上配置的hdfs的路径需要提前存在
>

创建路径: 
```shell script
sbin/start-dfs.sh
hadoop fs -mkdir /directory
```

## 修改spark-env

添加日志配置: 

```shell script
export SPARK_HSITORY_OPTS="
-Dspark.history.ui.port=18080
-Dspark.history.fs.logDirectory=hdfs://node1:8020/directory
-Dspark.history.retainedApplication=30
"
```
- 设置history UI访问端口
- 设置历史数据路径
- 历史记录的个数


> 在node2 node3 上做以上配置
>

## 重新提交任务

启动集群:  
```shell script
./sbin/start-all.sh
./sbin/start-history-server.sh
```

提交任务:   
```scala 3
sudo sh spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://node1:7077 \
../examples/jars/spark/spark-examples_2.12-3.0.0-preview2.jar \
5
```

执行结果和之前一样。  


## 查看历史执行记录

访问： http://localhost:18080

**视频地址：**   https://www.bilibili.com/video/BV11A411L7CK?p=15&spm_id_from=pageDriver



