
# spark-submit 参数说明


## 提交案例

```shell script
./bin/spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  ... # other options
  <application-jar> \
  [application-arguments]
```


## 简单介绍

```text
--class:  你所要运行的类
--master: 如spark://host:port, mesos://host:port, yarn,  yarn-cluster,yarn-client, local
--deploy-mode: 使用 client 还是 cluster 方式提交job 默认使用 client 区别在于 driver 位置 
--conf: 配置参数
application-jar:  java 的位置 如  hdfs:// path or a file:// path .
application-arguments: 传递给主类的主方法的参数
```


## 参数说明

| 参数名 | 参数说明 | 
| ---- | ---- |
| name | 应用程序的名称 |
| jars	| 逗号分隔的本地jar包，包含在driver和executor的classpath下 |
| packages	| 包含在driver和executor的classpath下的jar包逗号分隔的”groupId:artifactId：version”列表 | 
| repositories	| 逗号分隔的远程仓库 | 
| exclude-packages	| 为了避免冲突 而指定不包含的 package 用逗号分隔的”groupId:artifactId”列表 | 
| py-files	| 逗号分隔的”.zip”,”.egg”或者“.py”文件，这些文件放在python app的PYTHONPATH下面 | 
| files	| 逗号分隔的文件，这些文件放在每个executor的工作目录下面 | 
| conf	| 指定 spark 配置属性的值， 例如 -conf spark.executor.extraJavaOptions="-XX:MaxPermSize=256m"
properties-file	| 固定的spark配置属性，默认是conf/spark-defaults.conf | 
| driver-memory	| Driver内存，默认 1G | 
| driver-java-options	| 传给driver的额外的Java选项 例如 -XX:PermSize=128M -XX:MaxPermSize=256M | 
| driver-library-path | 传给driver的额外的库路径 | 
| driver-class-path | 传给 driver 的额外的类路径 | 
| proxy-user	| 模拟提交应用程序的用户 | 
| driver-cores	| Driver的核数，默认是1。这个参数仅仅在standalone集群deploy模式下使用 | 
| supervise	| Driver失败时，重启driver。在mesos或者standalone下使用 | 
| verbose	| 打印debug信息 | 
| total-executor-cores	| executor使用的总核数，仅限于Spark Alone、Spark on Mesos模式 | 
| executor-memory | 每个executor的内存，默认是1G | 
| executor-core	| 每个executor使用的内核数，默认为1 ，仅限于Spark on Yarn模式 | 
| num-executors	| 启动的executor数量。默认为2。在yarn下使用 | 
| queue	| 提交应用程序给哪个YARN的队列，默认是default队列，仅限于Spark on Yarn模式 | 
| archives	| 被每个executor提取到工作目录的档案列表，用逗号隔开 |



**视频链接:**  https://www.bilibili.com/video/BV11A411L7CK?p=14


 